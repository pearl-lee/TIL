# 고윳값분해, 특이값분해(복습), PCA, 함수강의

## 고윳값분해(고유분해)
- w, V = np.linalg.eig(A): w는 고윳값벡터, V는 고유벡터
- N차원 정방행렬 A에 대해
1. A는 N개의 고윳값, 고유벡터를 가진다
2. 대각합은 모든 고윳값들의 합
3. 행렬식은 모든 고윳값들의 곱
4. A가 대칭행렬
	- 고윳값은 N개의 실수
	- 고유벡터 서로 직교
	- 고유벡터가 선형 독립 -> 대각화 가능 
	- A가 양의 정부호 <-> 고윳값이 0보다 크다
	- A가 양의 준정부호 <-> 고윳값이 0보다 크거나 같다
5. A가 행렬 X의 분산행렬(X.T @ X)
	- A는 양의 준정부호
	- 고윳값은 0보다 크거나 같다
	- X가 풀랭크 -> A의 역행렬 존재

## 특이값분해
- U, S, VT = np.linalg.svd(A)
- A @ v_i = s_i @ u_i
- **특이분해와 고유분해의 관계**
- A의 분산행렬 A.T @ A
	- A의 특이값 제곱: A.T @ A의 고윳값
	- A의 오른쪽 특이벡터: A.T @ A의 고우벡터
- K차원 근사문제
	- N개의 M차원 벡터 A를 K차원 벡터공간에 투영
	- A와 가장 비슷한 K차원 벡터를 구하기 위한 기저벡터 찾기
	-> 가장 큰 K개의 특이값에 대응하는 오른쪽 특이벡터가 기저벡터일때, 그것이 찾는 값

## PCA 주성분분석
- 잠재변수와 측정 데이터가 선형관계라는 가정 하에서, 측정 데이터와 가중치 벡터를 선형조합해서 잠재변수 생성.
- 데이터 분석시, 선형종속인 데이터를 버리기 아까울 때 새로운 잠재변수를 생성해서 데이터를 압축, 저차원 데이터로 만들어서 사용

## 함수
- 로지스틱함수(시그모이드 함수)
	- 0보다 크고 1보다 작다
- 로그함수(데이터 분석에서 아주 중요)
1. 곱하기를 더하기로 변환
	- log(xy) = logx + logy
	- log(x ** n) = nlog(x)
2. 함수에 로그를 취해도 함수의 최고점, 최저점의 위치는 변하지 않음
3. 0을 -무한대로, 1을 0으로 변환해 간격을 확대시켜줌

- 인공신경망
	- ReLU(y=0으로 고정한 최대함수)
	- 소프트맥스함수: 0보다 크고 1보다 작음, 출력값의 합은 1