{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "- 머신러닝을 위한 오픈소스 플랫폼 : 딥러닝 프레임워크\n",
    "- 구글이 주도적으로 개발 : 구글 코랩에 기본 장착\n",
    "- 최근 2.x 버전 발표\n",
    "- Keras라고 하는 고수준 API 병합\n",
    "\n",
    "#### 텐서플로우\n",
    "- Tensor : 벡터나 행렬을 의미\n",
    "- Graph : 텐서가 흐르는 경로(혹은 공간)\n",
    "- Tensor Flow : 텐서가 Graph를 통해 흐른다\n",
    "\n",
    "## 딥러닝의 기초 (feat. Keras)\n",
    "### 신경망에서 아이디어를 얻어서 시작된 Neural Net\n",
    "#### 뉴런 \n",
    "- 신경망의 최소단위 (입력, 가중치, 활성화 함수, 출력으로 구성)\n",
    "     - 뉴런에서 학습할 때 변하는 것은 가중치.\n",
    "     - 처음에는 초기화를 통해 랜덤값을 넣고, 학습과정에서 일정한 값으로 수렴\n",
    "     \n",
    "#### 레이어와 망(net)\n",
    "- 뉴런이 모여 layer를 구성하고 망(net)이 됨\n",
    "\n",
    "#### 딥러닝\n",
    "- 신경망이 깊어(많아)지면 깊은 신경망 Deep Learning이 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 역전파(back-propagation)\n",
    "    - 뉴럴넷의 학습방법\n",
    "    - 현재 내가 틀린 정도를 '미분(기울기)' 한 것을 뒤로 전달\n",
    "    - 미분하고, 곱하고, 더하고 역방향으로 반복하며 업데이트 한다\n",
    "    \n",
    "- sigmoid 문제\n",
    "    - 우리가 activation함수로 sigmoid를 썼다는 것\n",
    "    - 기울기가 0인 곳을 중간에 곱하면, 뒤로 전달할 것이 없어진다..\n",
    "    - 이걸 반복하면?!\n",
    "    \n",
    "- gradient vanishing\n",
    "    - 레이어가 깊을수록 업데이트가 사라져간다.\n",
    "    - 그래서 fitting이 잘 안됨(underfitting)\n",
    "    \n",
    "- ReLU(Rectified Linear Units)\n",
    "    - 사그라드는 sigmoid 대신 죽지 않는 ReLU를 activation function으로 쓰자\n",
    "    - 양의 구간에서 전부 미분 값이 있다\n",
    "    \n",
    "- softmax\n",
    "    - 뉴럴넷에게 답을 회신받는 3가지 방법\n",
    "        - output을 그냥 받는다 (value)\n",
    "        - output에 sigmoid를 먹인다 (O or X)\n",
    "        - output에 softmadx를 먹인다 (Category)\n",
    "        \n",
    "- gradient decent\n",
    "    - 기존 뉴럴넷이 가중치 parameter들을 최적화 하는 방법\n",
    "    - loss function의 현 가중치에서 기울기를 구해서 loss를 줄이는 방향으로 업데이트 해나간다\n",
    "    \n",
    "- 뉴럴넷은 loss function(틀린 정도)를 가지고 있다\n",
    "    - 현재 가진 weight 세팅(내 자리)에서, 내가 가진 데이터를 다 넣으면 전체 에러가 계산됨\n",
    "    - 거기서 미분하면 에러를 줄이는 방향을 알 수 있다(내 자리 기울기 * 반대방향)\n",
    "    - 그 방향으로 정해진 스텝량(learning rate)을 곱해서 weight를 이동시킨다. 이걸 반복!\n",
    "    - weight의 업데이트 = 에러 낮추는 방향(decent; $-$) X 한발자국 크기(learning rate; $\\gamma$) X 현지점의 기울기(gradient; $\\nabla F(a^n)$)\n",
    "    - $-\\gamma \\nabla F(a^n)$ \n",
    "    \n",
    "- SGD(Stochastic Gradient Decent)\n",
    "    - GD : 전부 다 읽고나서 최적의 1스텝을 간다(full-batch)\n",
    "        - 모든 걸 계산(1시간) 후 최적의 한스텝. \n",
    "        - 최적인데 너무 느리다\n",
    "    - SGD : 작은 토막마다 1스텝을 간다(mini-batch)\n",
    "        - 일부만 검토(5분) 후 틀려도 일단 간다.\n",
    "        - 조금 헤메더라도 근처로 빠르게 갈 수 있다\n",
    "        \n",
    "- 옵티마이저의 선택\n",
    "    - $-\\gamma \\nabla F(a^n)$ 산을 잘 타고 내려오는 것은, \n",
    "    - $\\nabla F(a^n)$ 어떤 방향으로 발을 디딜지 + $\\gamma$ 얼마 보폭으로 발을 디딜지\n",
    "    - 두가지를 잘 잡아야 빠르게 타고 내려온다\n",
    "    - 잘 모르겠으면 일단 Adam을 써보자: SGD에서 방향과 스텝 사이즈를 적절하게 설정하도록 발전된? 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
