{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n",
    "- PCA : Principle Component 활용하여 차원을 줄이면서 정보의 손실을 최소화 하는 방법\n",
    "- PCA가 필요한 이유\n",
    "    - 차원이 증가함에 따라 모델의 복잡도가 기하급수적으로 높아지므로, 자료의 패턴을 설명하기 어려워짐\n",
    "    - KNN(K-nearest neighborhood) 모델: 가까이에 있는 변수가 가지는 값을 예측하는 모델\n",
    "        - 쓸데없는 변수가 추가되는 것은 모델의 성능에 악영향\n",
    "        - 상관계수가 매우 큰 서로 다른 독립변수, 예측하고자 하는 변수와 관련이 없는 변수 제거\n",
    "        - 그러나 정보의 손실이 발생하므로, 그것을 완화시키기 위해 PCA를 사용.\n",
    "---\n",
    "#### 공분산 행렬(Covariance matrix)\n",
    "- $X_1$, $X_2$가 상관관계를 가지고, $X$가 centering 되어 있다면\n",
    "- $Cov\\begin{pmatrix}X = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}\\end{pmatrix} = \\begin{bmatrix} Var(X_1) & Cov(X_1, X_2) \\\\ Cov(X_1, X_2) & Var(X_2) \\end{bmatrix}$\n",
    "\n",
    "- $Cov(X) = {(X^T X) \\over (n - 1)}$\n",
    "\n",
    "- 공분산의 형태 파악\n",
    "    - 점과 내적연산을 할 경우, 점의 위치를 이동시켜 해당 공분산 구조와 비슷한 형태를 가지게 됨\n",
    "    - Positiv Definite, 행렬식 $\\not=$ 0\n",
    "---\n",
    "#### Principle Components 개념\n",
    "- 차원을 줄이면서 정보의 손실을 최소화 하는 방법\n",
    "- 더 적은 개수로 데이터를 충분히 잘 설명할 수 있는 새로운 축을 찾아냄\n",
    "    - 공분산이 데이터의 형태를 변형시키는 방향의 축과 그것에 직교하는 축\n",
    "    - 2차원의 경우 공분산이 나타내는 타원의 장축과 단축\n",
    "    \n",
    "- Principle Components = PC = PC Score\n",
    "    - 새로운 축에서의 좌표값\n",
    "    - 새로운 축에 내린 정사영\n",
    "---\n",
    "#### PCA의 수학적 개념\n",
    "- 행렬식(Determinant) : $det A = |A|$\n",
    "    - 기하학적 의미\n",
    "        - $|A|$는 A의 선형변화의 스케일 성분을 의미\n",
    "        - $D' = AD$일 때, $area(D') = |A|area(D)$\n",
    "        - $|A|=0$인 경우, $D'$는 선분\n",
    "    - 행렬식과 역행렬의 존재성의 관계\n",
    "        - $|A| = 0$인 경우, 역행렬이 존재하지 않음\n",
    "        - 행렬식은 역행렬 존재성에 대한 판별식 역할\n",
    "- 고유벡터(Eigin vector), 고윳값(Eigin value)\n",
    "    - 정방행렬 A에 대하여, $Av = \\lambda v$ 를 만족하는 경우 \n",
    "        - $v$ : 고유벡터(eigen vector)\n",
    "        - $\\lambda$ : 고윳값(eigin value)\n",
    "    - $A$는 $v$를 선형변환 한다\n",
    "    - 임의의 점에 대하여 A라는 transformation을 행할 때 고유벡터는 방향이 바뀌지 않음\n",
    "    - 고윳값은 변화되는 스케일의 정도\n",
    "- 특이값 분해(SVD; Singular-Value Decomposition)\n",
    "    - $X = UDV^T$\n",
    "        - $U$ : n x p -> $U^T U = I_p$ - $X^T X$의 고유벡터\n",
    "        - $D$ : p x p -> 대각행렬\n",
    "        - $V$ : p x p -> $V^T V = I_p$ - $X^T X$의 고윳값\n",
    "    - 임의의 행렬 $X$의 공분산 구조 행렬($X^T X$)의 고유벡터, 고윳값을 구할 수 있음\n",
    "        - $X^T X = VD^2 V^T$ -> $(X^T X)V = VD^2$ \n",
    "        - $(X^T X)v_i = d_i^2 v_i (i=1,...,p)$\n",
    "            - $v_i$ : 고유벡터\n",
    "            - $d_i^2$ : 고윳값\n",
    "---\n",
    "#### PCA 수행과정\n",
    "1. Mean Centering\n",
    "2. SVD 수행\n",
    "    - $X = UDV^T$\n",
    "3. SVD의 결과를 활용하여 공분산의 고유벡터(eigen vector), 고윳값(eigen value)구하기\n",
    "4. PC Score 구하기\n",
    "    - $XV = UD$\n",
    "5. PC Score 활용하여 분석 진행\n",
    "    - PC Score를 설명변수로 활용하여 분석 진행\n",
    "    - $Y = \\begin{bmatrix} 1 & PC_1 & \\dots & PC_q \\end{bmatrix} \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_q \\end{bmatrix} + \\epsilon$\n",
    "    - $i$번째 관측치의 1번째 PC Score = $x_i^T v_1$\n",
    "        - 관측치 $a = x_i^T$와 고유벡터 $b = v_1$의 내적값\n",
    "---\n",
    "### Kernel PCA\n",
    "- 무슨말인지 아직 모르겠음..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
