{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다중 선형 회귀분석\n",
    "- $\\hat Y = \\hat\\beta_0 + \\hat\\beta_0 X_1 + \\dots + \\hat\\beta_p X_p$\n",
    "- 다중선형 회귀계수를 추정 : SSE를 최소화 하는 방향으로 추정\n",
    "    - $SSE = \\sum_{i = 1}^{n} e_i^2 = e_1^2 + e_2^2 + \\dots + e_n^2$\n",
    "    - 위 식을 각각의 변수($\\beta_0, \\dots, \\beta_p$)에 대해 편미분하여 회귀계수 추정(행렬)\n",
    "    - $y = X \\beta + \\epsilon$\n",
    "    - $\\epsilon = y - X \\beta$ -> ${\\partial \\sum_{i = 1}^{n}\\epsilon^2 \\over \\partial \\beta} = 0$ -> $\\beta = (X^T X)^{-1}X^T y$\n",
    "\n",
    "### 다중선형 회귀계수 해석 \n",
    "- p-value가 높으면 유의미한 영향을 미치지 못함\n",
    "    \n",
    "### 다중선형 회귀계수 검정\n",
    "- 단순 선형회귀분석과 같은 방법\n",
    "- 귀무가설 $\\beta_p = 0$, 대립가설 $\\beta_p \\not= 0$\n",
    "- $t = {\\hat\\beta_p \\over {s \\over \\sqrt{S_{xx}}}} = {\\hat\\beta_p \\over s.e(\\hat\\beta_p)}$\n",
    "\n",
    "### 다중선형 회귀모델 검정\n",
    "- 단순선형회귀에서는 변수가 1개이므로 회귀계수 검정결과와 모델 검정결과가 같다(할 필요가 없음)\n",
    "- 귀무가설 \n",
    "    - $\\beta_1 = \\beta_2 = \\dots = \\beta_p = 0$ \n",
    "    - 모든 회귀계수는 0이다. 즉 변수의 설명력이 전혀 없다\n",
    "    - 기각하기 쉬운 가설\n",
    "- 대립가설\n",
    "    - 하나의 회귀계수라도 0이 아니다\n",
    "    - 설명력이 있는 변수가 존재한다\n",
    "- F 검정\n",
    "    - $F = {V_1 / k_1 \\over V_2 / k_2} \\sim F(k_1, k_2)$\n",
    "    - 두 확률변수 $V_1, V_2$가 서로 독립인 $\\chi^2$분포를 따른다고 할 때, 확률변수 F는 F분포를 따름\n",
    "    - MSR, MSE가 모두 제곱합의 형태이므로 F검정 사용\n",
    "    - F 통계량이 커지면 R-square 값은 커지고 P-value는 작아짐\n",
    "        - 기각하기가 쉬워진다\n",
    "    - 변수가 많을 때 다중공선성에 의해, 결과를 신뢰할 수 없음\n",
    "\n",
    "| Source of Variation | Sum of Squares | Degree of Freedom | Mean Square | F-statistics | P-value |\n",
    "|------|------|------|------|------|------|\n",
    "| Regression | SSR | p | MSR (SSR/p) | MSR / MSE | P-value |\n",
    "| Error | SSE | N-p-1 | MSE(SSE/(N-p-1)) | | |\n",
    "| Total | SST | N-1 ||||\n",
    "\n",
    "---\n",
    "### 다중공선성(Multicollinearity)\n",
    "- 독립변수들이 강한 선형관계에 있는 경우\n",
    "- 종속변수 Y의 변동성(분산)은 변하지 않지만 독립변수 X가 많아지면 Y를 설명하는 변동성이 겹치게 됨\n",
    "- 겹치는 변동성에 대해서는 중복으로 가져가지 못함\n",
    "    - 따라서 변동성이 낮아지므로 회귀계수가 작아지게 됨\n",
    "    - 잘못된 변수해석, 예측 정확도 하락의 문제 발생\n",
    "    \n",
    "- 다중공선성 진단 방법\n",
    "    - VIF(Variance Inflation Factor), 변수들간의 Correlation 등으로 진단\n",
    "        - $VIF_i = {1\\over 1 - R_i^2}$\n",
    "        - VIF가 10이상인 경우 다중공선성이 있는 변수라고 판단\n",
    "            1. $X_1$을 종속변수, 나머지 변수들을 독립변수로 하여 회귀모델($f_1$) 적합\n",
    "                - $x_1 = \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon$\n",
    "            2. $f_1$의 $R^2$를 이용하여 $VIF_1$ 계산\n",
    "                - $VIF_1 = {1 \\over 1 - R_1^2}$\n",
    "                - 다른 변수의 선형결합으로 $X_1$을 설명할 수 잇는 정도\n",
    "            3. $R^2 > 0.9$인 경우, $VIF > 10$\n",
    "            \n",
    "- 다중공선성 해결 방법\n",
    "    - Feature Selection : 중요한 변수만 선택하는 방법\n",
    "        - correlation 등의 지표를 보고 단순히 변수를 제거하는 방법(산점도를 보고 삭제할지 결정해야 함)\n",
    "        - Lasso\n",
    "        - Stepwise\n",
    "        - 기타 알고리즘\n",
    "    - 변수를 줄이지 않고 활용하는 방법\n",
    "        - PCA\n",
    "        - Ridge\n",
    "        - AutoEncoder 등의 Feature Extraction 기법(딥러닝)\n",
    "    - 아직까지 근본적인 해결 방법은 없다\n",
    "---\n",
    "### 회귀모델의 성능지표\n",
    "- **$R^2$**\n",
    "    - 변수가 증가할 수록 $R^2$도 증가\n",
    "    - $R^2 = {SSR \\over SST} = 1 - {SSE\\over SST}$\n",
    "        - SST : Y의 변동성\n",
    "        - SST : Y를 설명하는 X의 변동성\n",
    "        - SSE : X를 통해 Y를 설명하지 못하는 변동성\n",
    "- **$Adjusted R^2$**\n",
    "    - $R^2$는 변수가 많아질수록 커지므로 성능지표로서 큰 의미가 없음\n",
    "    - $R^2$에 변수 수만큼 penalty를 주는 지표\n",
    "    - $Adjusted R^2 = 1 - {SSE / (n-p) \\over SST / (n-1)}$ \n",
    "    - $p$가 증가할수록 감소\n",
    "    \n",
    "- **AIC(Akaike Information Criterion)**\n",
    "    - MSE에 변수 수만큼 penalty를 주는 지표\n",
    "    - 일반적으로 회귀분석에서 Model Selection할 때 많이 쓰이는 지표\n",
    "    - $AIC = n ln \\begin{pmatrix}{SSE \\over n}\\end{pmatrix} + 2(p + 1)$\n",
    "    - $p$가 증가할수록 증가\n",
    "    \n",
    "- **BIC(Bayes Information Criteria) **\n",
    "    - AIC의 단점인 표본 n이 커질 때 부정확해지는 문제를 보완한 지표\n",
    "    - $BIC = n \\ln\\begin{pmatrix}{SSE \\over n}\\end{pmatrix} + (p + 1)\\ln n$ \n",
    "---\n",
    "### 모형의 성능지표\n",
    "- **MSE(Mean Squared Error)**\n",
    "    - $f$가 제대로 추정되었는지 평가하기 위해, 예측한 값이 실제값과 유사한지 평가하는 척도가 필요\n",
    "        $$MSE = {1\\over n}\\sum_{i=1}^{n}[y_i - \\hat f(x_i)]^2$$\n",
    "    - MSE는 실제 종속변수($y_i$)와 예측한 종속변수($\\hat f(x_i)$)간의 차이\n",
    "    - MSE가 작을 수록 좋지만, MSE를 과도하게 줄이면 과적합의 오류를 범할 가능성이 높아짐\n",
    "    - 따라서, 검증집합의 MSE를 줄이는 방향으로 $f$를 추정\n",
    "- **MAPE(Mean Absolute Percentage Error)**\n",
    "    - $f$가 제대로 추정되었는지 평가하기 위해, 예측한 값이 실제값과 유사한지 평가하는 척도가 필요\n",
    "         $$MAPE = {100\\over n}\\sum_{i=1}^{n}\\left|{y_i - \\hat f(x_i) \\over y_i}\\right|$  $(y_i \\not= 0)$$\n",
    "    - MAPE는 퍼센트 값을 가지며, 0에 가까울수록 회귀모형의 성능이 좋다고 해석할 수 있음\n",
    "    - 0% ~ 100% 사이의값을 가져 이해하기 쉬우므로 성능 비교해석이 가능\n",
    "---\n",
    "- **정확도(Accuracy)**\n",
    "    - 혼동행렬(confusion matrix)\n",
    "        $$\\begin{pmatrix} & 예측정상 & 예측 불량 \\\\ 실제정상 & TN & FP \\\\ 실제불량 & FN & TP \\end{pmatrix}$$\n",
    "    - 정확도는 전체 데이터 중에서 모형으로 판단한 값이 실제값과 부합하는 비율\n",
    "    - 분모는 전체 데이터가 괴고 분자는 모형이 실제 정상을 정상으로, 그리고 실제 이상을 이상으로 옳게 분류한 데이터\n",
    "    $$Accuracy = {옳게 분류된 데이터의 수 \\over 전체 데이터의 수} = {TP + TN \\over TP + FN + FP + TN}$$\n",
    "    \n",
    "- **정밀도(Precision)**\n",
    "    - 정밀도는 분류 모형이 불량을 진단하기 위해 얼마나 잘 작동했는지 보여주는 지표\n",
    "    $$Precision = {옳게 분류된 불량 데이터의 수 \\over 불량으로 예측한 데이터의 수} = {TP \\over FP + TP}$$\n",
    "\n",
    "- **재현율(Recall)**\n",
    "    - 재현율을 불량 데이터 중 실제로 불량이라고 진단한 제품의 비율 (진단확률)\n",
    "    $$Recall = {옳게 분류된 불량 데이터의 수 \\over 실제 불량 데이터의 수} = {TP \\over FN + TP}$$\n",
    "\n",
    "- **특이도(Specificity)**\n",
    "    - 특이도는 분류 모형이 정상을 진단하기 위해 잘 작동하는지 보여주는 지표\n",
    "    $$Specificity = {옳게 분류된 정상 데이터의 수 \\over 실제 정상 데이터의 수} = {TN \\over TN + FP}$$\n",
    "- **G-mean, $F_1$ measure**\n",
    "    - 실제 데이터의 대표적인 특성\n",
    "        - 불량(이상) 데이터를 탐지하는 것이 중요하다\n",
    "        - 이러한 불량 데이터는 매우 소수의 데이터(class inbalanced 문제)\n",
    "    - 데이터 1000개 중 불량 데이터라 10개, 나머지 990개는 정상데이터라고 가정.\n",
    "        - 분류모형이 모든 데이터를 정상데이터라고만 예측해도 정확도는 99%(accuracy paradox)\n",
    "        - 우연히 1개만 불량이라고 예측했는데, 실제 불량일 경우 정밀도 지표는 1임\n",
    "    - G-mean\n",
    "        - 실제 데이터의 특성상 정확도보다는 제1종 오류와 제2종 오류 중 성능이 나쁜 쪽에 더 가중치를 주는 지표\n",
    "        $$G-mean = \\sqrt{specificity \\cdot recall} = \\sqrt{(1 - \\alpha)\\cdot(1 - \\beta)}$$\n",
    "    - $F_1$ measure \n",
    "        - 불량에 관여하는 지표인 정밀도와 재현율만 고려하는 지표\n",
    "        $$F_1 measure = {2\\over {1\\over precision}+{1\\over recall}} = 2 \\cdot {precision \\cdot recall \\over precision + recall}$$\n",
    "- **ROC curve, AUC**\n",
    "    - ROC(Receiver Operating Characteristics) curve\n",
    "        - 가로축을 `1 - specificity`, 세로축을 `1 - recall`로 하여 시각화한 그래프\n",
    "        - 모델이 정확할 수록 1에 가까워지고, 그렇지 않을 수록 $y = x$에 가까워짐\n",
    "    - AUC\n",
    "        - ROC curve의 면적\n",
    "--- \n",
    "### 변수선택법\n",
    "- 변수가 여러개일 때 최선의 변수 조합을 찾아내는 방법\n",
    "- 변수가 p개일 때 변수의 총 조합은 $2^p$로 변수 수가 증가함에 따라 변수의 조합은 기하급수적으로 증가\n",
    "- 총 변수들의 조합 중 최적의 조합을 찾기위한 차선의 방법\n",
    "\n",
    "#### Feedforward Selection 방법\n",
    "- 변수를 추가해가며 성능지표를 비교\n",
    "- 변수를 최소한으로 쓰고 싶을 때\n",
    "- 변수를 하나씩 추가하며 AIC를 구하고, 감소하다가 증가하기 전의 변수 조합을 사용\n",
    "\n",
    "#### Backward Elimination 방법\n",
    "- 변수를 제거해 가며 성능지표를 비교\n",
    "- 전체 변수에서 하나씩 제거하며 AIC를 구하고, 감소하다가 증가하기 전의 변수 조합 사용\n",
    "\n",
    "#### Stepwise 방법\n",
    "- 가장 유의한 변수를 추가하거나 유의하지 않은 변수를 제거해나가는 방법\n",
    "- 전진선택법을 사용할 때 한 변수가 선택되면 이미 선택된 변수 중 중요하지 않은 변수가 있을 수 있음\n",
    "- 전진선택법의 각 단계에서 이미 선택된 변수들의 중요도를 다시 검사하여 중요하지 않은 변수를 제거하는 방법\n",
    "- 일반적으로 가장 널리 쓰임\n",
    "    1. 변수 입력/제거를 위해 P-value 임계치를 설정\n",
    "    2. Forward Selection을 통한 변수 선정\n",
    "    3. 선택된 변수 중 유의미한 변수를 남기고 제거, 2~3번 반복\n",
    "    4. 변수가 추가되거나 제거할 케이스가 없는 경우 종료\n",
    "\n",
    "### 교호작용(Interaction term)\n",
    "- 변수간의 시너지 효과\n",
    "    - $X_1$과 $X_2$는 $Y$에 영향을 끼치지는 않지만, $X_1$과 $X_2$가 결함됨으로써 $Y$에 중요한 영향을 끼칠 수 있음\n",
    "- $X_1$, $X_2$ 그리고 $X_1$과 $X_2$의 교호작용에 대해서 회귀 모델 방정식은 다음과 같이 쓸 수 있음\n",
    "$$\\hat Y = \\hat\\beta_0 + \\hat\\beta_1 X_1 + \\hat\\beta_2 X_2 + \\hat\\beta_3 X_1 X_2$$\n",
    "- 교호작용은 일반적으로 도메인지식에 근거하여 추가하여야 함\n",
    "---\n",
    "### 회귀분석의 진단\n",
    "- 회귀분석에서 잔차에 대한 세가지 가정(**정규성, 독립성, 등분산성**)이 존재하는데, 이 세가지 가정을 만족할 시 잘 만들어진 회귀모델이라 판단\n",
    "- 회귀모델을 잘 만들었을 때, 잔차는 정규분포를 따름\n",
    "    - Residuals 산점도 : 독립성\n",
    "    - Normal Q-Q plot : 정규성\n",
    "    - Residual vs. fitted plot : 등분산성\n",
    "- 잔차가 가정에 위배된 경우 해결방법\n",
    "    - Y에 대하여 log 또는 root를 씌워줌 (선형회귀적합)\n",
    "    - 이상치 제거 (신중해야함)\n",
    "    - 다항회귀분석 (비선형 회귀) : 3차항 이상에서는 overfitting의 가능성이 높아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
