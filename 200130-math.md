# PCA, Function review

## PCA(주성분분석), 차원축소
- 주어진 고차원(M-d) 데이터 집합과 가장 비슷한 낮은 차원(K-d)의 데이터를 찾아내는 방법(M > K)
- 로우랭크 근사문제로 풀이
- sklearn.decomposition 패키지의 PCA 클래스 이용
	- pca = PCA(n_components=K)

	- M차원 데이터X를 K차원 데이터로 선형변환
	- X_K = pca.fit_transform(X) 
	- 비교를 위해 M차원 데이터로 역변환
	- X_KM = pca.inverse_transform(X_K)
	- pca.mean_: X데이터의 평균 벡터
	- pca.components_: 주성분 벡터
		- 가장 큰 특이값에 대응하는 오른쪽 특이벡터
		- 분산행렬의 가장 큰 고윳값에 대응하는 고유벡터

## 함수
- ReLU(Rectified Linear Unit): 인공신경망에서 쓰이는 최대함수(y=0으로 고정)
- 로지스틱 함수
	- 시그모이드함수의 한 중류. 0보다 크고 1보다 작음
	- (1 + exp(-x))^(-1)
- 로그함수
	- 지수함수의 역함수
	- log(a): e를 거듭제곱하여 a가 되도록 하는 수
	- 확률론에서 가장 많이 사용
		- log(xy) = logx + logy
		- argmax f(x) = argmax logf(x)
		- 0과 1 사이의 값을 확대해서 보여줌
- 소프트플러스 함수
	- log(1 + exp(x))
- 소프트맥스 함수
	- 다변수 다출력 함수
	- 출력값은 0과 1사이
	- 모든 출력값의 합은 1
	- 다변수 입력을 확률처럼 보이게 출력

# Sympy 미적분 강의

## 미분
- 최적화
- sympy.diff(f)

## 적분
- 확률론 확률계산
- 부정적분
	- F = sympy.integrate(f)
- 정적분
	- 부정적분을 이용한 정적분
	- (F.subs(x, b) - F.subs(x, a)) 
	- 수치적분
	- sp.integrate.quad(func, a, b)
	- 다변수 정적분
	- sp.integrate.dblquad(func, a, b, gfun, hfun)
	- gfun, hfun: y값의 하한, 상한(함수로 작성)

## 행렬의 미분
